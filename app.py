from flask import Flask, config, render_template, request, flash, jsonify, send_from_directory, abort, Response, send_file
from flask_cors import CORS
import paramiko
import subprocess
import re
import ast
from threading import Lock
import json
import threading
import time
import os
import requests
from io import BytesIO

app = Flask(__name__)
CORS(app)

deploy_lock = Lock()

auto_deploy_status = {
    'status': 'idle',  # idle, running, done, error
    'step': 0,
    'total_steps': 6,
    'progress': 0,
    'message': '',
    'log': [],
    'cluster_links': {},
    'steps': []
}

semi_auto_deploy_status = {
    'status': 'idle',  # idle, running, done, error
    'step': 0,
    'total_steps': 6,
    'progress': 0,
    'message': '',
    'log': [],
}

WEBHDFS_URL = 'http://localhost:9870/webhdfs/v1'
WEBHDFS_USER = 'hadoop'  # å¯æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/toolchain')
def toolchain():
    return render_template('toolchain.html')

@app.route('/deploy/start')
def start():
    return render_template('start.html')

@app.route('/deploy/method')
def deploy_method():
    return render_template('components/deploy-method.html')

@app.route('/deploy/auto')
def deploy_auto():
    return render_template('components/deploy-auto.html')

@app.route('/deploy/semiAutoConfig')
def deploy_semi_auto():
    return render_template('components/deploy-semi-auto.html')

@app.route('/deploy/semi-auto/progress')
def deploy_semi_auto_progress():
    return render_template('components/deploy-semi-auto-progress.html')

@app.route('/deploy/manualConfig')
def deploy_manual():
    return render_template('components/deploy-manual.html')

@app.route('/deploy/manual/progress')
def deploy_manual_progress():
    return render_template('components/deploy-manual-progress.html')

@app.route('/privacy-policy')
def privacy_policy():
    return render_template('privacy-policy.html')

@app.route('/terms-of-service')
def terms_of_service():
    return render_template('terms-of-service.html')

@app.route('/documentation')
def documentation():
    return render_template('documentation.html')

@app.route('/about')
def about():
    return render_template('about.html')

@app.route('/hdfs-manager')
def hdfs_manager():
    return render_template('hdfs-manager.html')   

@app.route('/api/scan_hosts', methods=['POST'])
def scan_hosts():
    try:
        request_data = request.get_json()
        subnet = request_data.get('subnet', '')
        if not subnet:
            return jsonify({"success": False, "error": "ç¼ºå°‘ç½‘æ®µå‚æ•°"}), 400
        result = subprocess.run(f"nmap -sn {subnet}", shell=True, capture_output=True, text=True)
        output = result.stdout

        #è·å–ipåœ°å€
        hosts = []
        for line in output.split('\n'):
            ip_match = re.match(r'Nmap scan report for (.+?)( \(([\d\.]+)\))?$', line)
            if ip_match:
                ip = ip_match.group(3) if ip_match.group(3) else ip_match.group(1)
                hosts.append(ip)

        return jsonify({"success": True, "hosts": hosts})
    except Exception as e:
        print(f"æ‰«æå¼‚å¸¸: {e}")
        return jsonify({"success": False, "error": str(e)}), 500

def servers_info():
    servers = request.get_json().get('servers', [])
    return servers

def ssh_client(hostname,username,password,port):
    ssh = paramiko.SSHClient()
    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
    ssh.connect(hostname,username=username,password=password,port=port)
    return ssh

@app.route('/api/yum/configure',methods=['POST'])
def configure_yum():
    servers = servers_info()
    results = []
    for server in servers:
        ssh = ssh_client(server['hostname'], server['username'], server['password'], int(server.get('port', 22)))
        stdin, stdout, stderr = ssh.exec_command("cat /etc/redhat-release")
        version_info = stdout.read().decode('utf-8').strip()
        if "8" in version_info:
            stdin,stdout,stderr = ssh.exec_command("rm -rf /etc/yum.repos.d/* && curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-8.repo")
            exit_code = stdout.channel.recv_exit_status()
            if exit_code == 0:
                ssh.exec_command("yum clean all && yum makecache")
        else:
            stdin,stdout,stderr = ssh.exec_command("rm -rf /etc/yum.repos.d/* && curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo")
            exit_code = stdout.channel.recv_exit_status()
            if exit_code == 0:
                ssh.exec_command("yum clean all && yum makecache")
        ssh.close()
        results.append({
            "hostname": server['hostname'],
            "host": server['hostname'],
            "success": True,
            "status": "success"
        })
    return jsonify({"status": "completed", "results": results})

def set_hostname(servers):

    server_count = 0
    for server in servers:
        server_count += 1
        if server_count <= 1:
            hostname = 'master'
        elif server_count <= len(servers):
            hostname = f'slave{server_count - 1}'
        
        ssh = ssh_client(server['hostname'], server['username'], server['password'], int(server.get('port', 22)))
        ssh.exec_command(f"hostnamectl set-hostname {hostname}")


def close_firewall(ssh):
    stdin,stdout,stderr = ssh.exec_command("systemctl stop firewalld && systemctl disable firewalld")
    stdout.channel.recv_exit_status()




@app.route('/api/deploy/auto/start', methods=['POST'])
def api_deploy_auto_start():
    with deploy_lock:
        if not request.json:
            return jsonify({'msg': 'è¯·æ±‚ä½“ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯'}), 400
        config = request.json.get('config')
        # å…¼å®¹å‰ç«¯ä¼ é€’listã€dictæˆ–str
        if isinstance(config, str):
            try:
                config = json.loads(config)
            except Exception:
                return jsonify({'msg': 'configå‚æ•°æ ¼å¼é”™è¯¯'}), 400
        if config is None:
            return jsonify({'msg': 'ç¼ºå°‘configå‚æ•°'}), 400
        if auto_deploy_status['status'] == 'running':
            return jsonify({'msg': 'å·²æœ‰éƒ¨ç½²åœ¨è¿›è¡Œä¸­'}), 400
        # é‡ç½®çŠ¶æ€
        auto_deploy_status['status'] = 'idle'
        auto_deploy_status['step'] = 0
        auto_deploy_status['progress'] = 0
        auto_deploy_status['message'] = ''
        auto_deploy_status['log'] = []
        t = threading.Thread(target=auto_deploy_task, args=(config,))
        t.start()
    return jsonify({'msg': 'éƒ¨ç½²å·²å¯åŠ¨'})

@app.route('/api/deploy/semi-auto/start', methods=['POST'])
def api_deploy_semi_auto_start():
    with deploy_lock:
        if not request.json:
            return jsonify({'success': False, 'msg': 'è¯·æ±‚ä½“ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯'}), 400
        config = request.json.get('config')
        if isinstance(config, str):
            try:
                config = json.loads(config)
            except Exception:
                return jsonify({'success': False, 'msg': 'configå‚æ•°æ ¼å¼é”™è¯¯'}), 400
        if config is None:
            return jsonify({'success': False, 'msg': 'ç¼ºå°‘configå‚æ•°'}), 400
        if semi_auto_deploy_status['status'] == 'running':
            return jsonify({'success': False, 'msg': 'å·²æœ‰éƒ¨ç½²åœ¨è¿›è¡Œä¸­'}), 400

        # è§£æå‚æ•°
        basic = config.get('basic', {})
        cluster = config.get('cluster', {})
        performance = config.get('performance', {})
        components = config.get('components', {})
        advanced = config.get('advanced', {})

        # ç¤ºä¾‹ï¼šè·å–å…·ä½“å­—æ®µ
        hadoop_version = basic.get('hadoopVersion')
        java_version = basic.get('javaVersion')
        namenode_host = basic.get('namenodeHost')
        # ... å…¶ä»–å‚æ•°åŒç†

        # print('Hadoopç‰ˆæœ¬:', hadoop_version)
        # print('Javaç‰ˆæœ¬:', java_version)
        # print('ä¸»èŠ‚ç‚¹:', namenode_host)
        # print('DataDir:', basic.get('dataDir'))
        # print('Datanodeæ•°é‡:', cluster.get('datanodeCount'))
        # ... å…¶ä»–å‚æ•°æ‰“å°

        # é‡ç½®çŠ¶æ€
        semi_auto_deploy_status['status'] = 'idle'
        semi_auto_deploy_status['step'] = 0
        semi_auto_deploy_status['progress'] = 0
        semi_auto_deploy_status['message'] = ''
        semi_auto_deploy_status['log'] = []
        t = threading.Thread(target=semi_auto_deploy_task, args=(config,))
        t.start()
    return jsonify({'success': True, 'msg': 'éƒ¨ç½²å·²å¯åŠ¨'})


def generate_core_site(master_ip, hadoop_tmp_dir="/data/hadoop/tmp"):
    return f"""<?xml version="1.0"?>
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://{master_ip}:9000</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>{hadoop_tmp_dir}</value>
        <description>Hadoopä¸´æ—¶ç›®å½•ï¼Œæ‰€æœ‰èŠ‚ç‚¹ï¼ˆåŒ…æ‹¬DataNodeï¼‰éƒ½éœ€è¦</description>
    </property>
    <!-- å¯é€‰ï¼šå¢åŠ å…¶ä»–DataNodeç›¸å…³å‚æ•° -->
    <property>
        <name>io.file.buffer.size</name>
        <value>131072</value>
    </property>
    <property>
        <name>fs.trash.interval</name>
        <value>1440</value>
        <description>å›æ”¶ç«™ä¿ç•™æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰</description>
    </property>
</configuration>
"""

def generate_hdfs_site(replication=3, namenode_dir="/opt/hadoop/data/dfs/namenode", datanode_dir="/opt/hadoop/data/dfs/datanode"):
    return f"""<?xml version="1.0"?>
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>{replication}</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:{namenode_dir}</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:{datanode_dir}</value>
    </property>
    <property>
        <name>dfs.webhdfs.enabled</name>
        <value>true</value>
    </property>
</configuration>
"""

def generate_yarn_site(resourcemanager_ip, yarn_memory=4096, yarn_cores=4):
    return f"""<?xml version="1.0"?>
<configuration>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>{resourcemanager_ip}</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>{yarn_memory}</value>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>{yarn_memory}</value>
    </property>
    <property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <value>{yarn_cores}</value>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-vcores</name>
        <value>{yarn_cores}</value>
    </property>
</configuration>
"""

def generate_mapred_site(map_memory=2048, map_cores=2):
    return f"""<?xml version="1.0"?>
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>mapreduce.map.memory.mb</name>
        <value>{map_memory}</value>
    </property>
    <property>
        <name>mapreduce.reduce.memory.mb</name>
        <value>{map_memory}</value>
    </property>
    <property>
        <name>mapreduce.map.cpu.vcores</name>
        <value>{map_cores}</value>
    </property>
    <property>
        <name>mapreduce.reduce.cpu.vcores</name>
        <value>{map_cores}</value>
    </property>
</configuration>
"""

def generate_workers(servers, datanodeCount=3):
    workers = []
    for server in servers:
        workers.append(server['hostname'])
    if len(workers) < datanodeCount:
        datanodeCount = len(workers)
    return workers

def get_node_color(server_index, total_servers):
    """ä¸ºä¸åŒèŠ‚ç‚¹ç”Ÿæˆä¸åŒçš„é¢œè‰²"""
    colors = [
        '#FF6B6B',  # çº¢è‰² - ä¸»èŠ‚ç‚¹
        '#4ECDC4',  # é’è‰² - ä»èŠ‚ç‚¹1
        '#45B7D1',  # è“è‰² - ä»èŠ‚ç‚¹2
        '#96CEB4',  # ç»¿è‰² - ä»èŠ‚ç‚¹3
        '#FFEAA7',  # é»„è‰² - ä»èŠ‚ç‚¹4
        '#DDA0DD',  # ç´«è‰² - ä»èŠ‚ç‚¹5
        '#98D8C8',  # è–„è·ç»¿ - ä»èŠ‚ç‚¹6
        '#F7DC6F',  # é‡‘é»„è‰² - ä»èŠ‚ç‚¹7
        '#BB8FCE',  # æ·¡ç´«è‰² - ä»èŠ‚ç‚¹8
        '#85C1E9'   # å¤©è“è‰² - ä»èŠ‚ç‚¹9
    ]
    return colors[server_index % len(colors)]

def format_node_log(step, message, server_ip=None, server_index=None, total_servers=None):
    """æ ¼å¼åŒ–èŠ‚ç‚¹æ—¥å¿—ï¼Œä¸ºä¸åŒèŠ‚ç‚¹æ·»åŠ é¢œè‰²"""
    if server_ip and server_index is not None and total_servers is not None:
        color = get_node_color(server_index, total_servers)
        return f"{step} <span style='color: {color}; font-weight: bold;'>{server_ip}</span> {message}"
    else:
        return f"{step} {message}"

def auto_deploy_task(config):
    auto_deploy_status['status'] = 'running'
    auto_deploy_status['log'] = []
    auto_deploy_status['steps'] = [
        {'name': 'ç¯å¢ƒæ£€æµ‹', 'status': 'pending'},
        {'name': 'å®‰è£…Javaç¯å¢ƒ', 'status': 'pending'},
        {'name': 'ä¸‹è½½å¹¶è§£å‹Hadoop', 'status': 'pending'},
        {'name': 'è‡ªåŠ¨é…ç½®Hadoopæ ¸å¿ƒå‚æ•°', 'status': 'pending'},
        {'name': 'å¯åŠ¨Hadoopé›†ç¾¤æœåŠ¡', 'status': 'pending'},
        {'name': 'éªŒè¯é›†ç¾¤è¿è¡ŒçŠ¶æ€', 'status': 'pending'}
    ]
    if isinstance(config, str):
        config = ast.literal_eval(config)
    servers = config.get('servers', []) if isinstance(config, dict) else config
    auto_deploy_status['log'].append("--------------------------------å…¨è‡ªåŠ¨éƒ¨ç½²å¼€å§‹--------------------------------")
    auto_deploy_status['step'] = 1
    auto_deploy_status['progress'] = 10
    auto_deploy_status['log'].append("[1/6] ğŸš€ å¼€å§‹ç¯å¢ƒæ£€æµ‹ä¸SSHå…å¯†é…ç½®...")
    auto_deploy_status['steps'][0]['status'] = 'doing'

    set_hostname(servers)
    pubkey_list = []
    for i, server in enumerate(servers):
        ip = server['hostname']
        username = server['username']
        password = server['password']
        port = int(server.get('port', 22))
        auto_deploy_status['log'].append(format_node_log("[1/6] ğŸ”— æ­£åœ¨è¿æ¥èŠ‚ç‚¹", f"(ç”¨æˆ·: {username}, ç«¯å£: {port})...", ip, i, len(servers)))
        ssh = ssh_client(ip, username, password, port)
        auto_deploy_status['log'].append(format_node_log("[1/6] âœ… èŠ‚ç‚¹", "SSHè¿æ¥æˆåŠŸï¼Œå¼€å§‹é…ç½®SSHå¯†é’¥...", ip, i, len(servers)))
        ssh.exec_command("ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa")
        #ç­‰å¾…ssh-keygenç”Ÿæˆå…¬é’¥
        time.sleep(1)
        #è·å–å…¬é’¥
        stdin, stdout, stderr = ssh.exec_command("cat ~/.ssh/id_rsa.pub")
        pubkey = stdout.read().decode().strip()
        #æ·»åŠ å…¬é’¥åˆ°pubkey_list
        pubkey_list.append(pubkey)
        #æ·»åŠ ipæ˜ å°„åˆ°hosts
        server_count = 0
        for server2 in servers:
            server_count += 1
            if server_count <= 1:
                hostname = 'master'
            elif server_count <= len(servers):
                hostname = f'slave{server_count - 1}'
            ssh.exec_command(f"echo '{server2['hostname']}\t{hostname}' >> /etc/hosts")
            #å…³é—­é˜²ç«å¢™
            close_firewall(ssh)
        auto_deploy_status['log'].append(format_node_log("[1/6] ğŸ”§ èŠ‚ç‚¹", "ä¸»æœºåæ˜ å°„å’Œé˜²ç«å¢™é…ç½®å®Œæˆ", ip, i, len(servers)))
    
    #æ·»åŠ å…¬é’¥åˆ°authorized_keys
    auto_deploy_status['log'].append("[1/6] ğŸ”‘ æ­£åœ¨é…ç½®SSHå…å¯†ç™»å½•...")
    for server in servers:
        ssh = ssh_client(server['hostname'], server['username'], server['password'], int(server.get('port', 22)))
        for pubkey in pubkey_list:
            ssh.exec_command(f"echo '{pubkey}' >> ~/.ssh/authorized_keys")
        ssh.close()

    auto_deploy_status['log'].append(f"[1/6] âœ… ç¯å¢ƒæ£€æµ‹å®Œæˆï¼å…±é…ç½® {len(servers)} ä¸ªèŠ‚ç‚¹")
    auto_deploy_status['steps'][0]['status'] = 'done'

    
    auto_deploy_status['progress'] = 20
    auto_deploy_status['step'] = 2
    auto_deploy_status['steps'][1]['status'] = 'doing'
    auto_deploy_status['log'].append("[2/6] â˜• å¼€å§‹å®‰è£…Javaè¿è¡Œç¯å¢ƒ...")
    
    for i, server in enumerate(servers):
        ssh = ssh_client(server['hostname'], server['username'], server['password'], int(server.get('port', 22)))
        auto_deploy_status['log'].append(format_node_log("[2/6] ğŸ” æ£€æŸ¥èŠ‚ç‚¹", "çš„Javaç¯å¢ƒ...", server['hostname'], i, len(servers)))
        stdin, stdout, stderr = ssh.exec_command("which java")
        java_path = stdout.read().decode().strip()
        if not java_path:
            auto_deploy_status['log'].append(format_node_log("[2/6] ğŸ“¦ èŠ‚ç‚¹", "æœªæ£€æµ‹åˆ°Javaï¼Œå¼€å§‹å®‰è£…OpenJDK 8...", server['hostname'], i, len(servers)))
            stdin, stdout, stderr = ssh.exec_command("yum install -y java-1.8.0-openjdk-devel", timeout=120)
            auto_deploy_status['log'].append(format_node_log("[2/6] â³ èŠ‚ç‚¹", "æ­£åœ¨å®‰è£…Javaï¼Œè¯·ç¨å€™...", server['hostname'], i, len(servers)))
            exit_code = stdout.channel.recv_exit_status()
            if exit_code != 0:
                auto_deploy_status['log'].append(format_node_log("[2/6] âŒ èŠ‚ç‚¹", "å®‰è£…Javaå¤±è´¥ï¼Œè¯·æ£€æŸ¥yumæºå’Œæƒé™", server['hostname'], i, len(servers)))
                auto_deploy_status['steps'][1]['status'] = 'error'
                return
            auto_deploy_status['log'].append(format_node_log("[2/6] âœ… èŠ‚ç‚¹", "Javaå®‰è£…æˆåŠŸ", server['hostname'], i, len(servers)))
        else:
            auto_deploy_status['log'].append(format_node_log("[2/6] âœ… èŠ‚ç‚¹", f"å·²å­˜åœ¨Javaç¯å¢ƒ: {java_path}", server['hostname'], i, len(servers)))
        ssh.close()
    
    auto_deploy_status['log'].append(f"[2/6] âœ… Javaç¯å¢ƒå®‰è£…å®Œæˆï¼å…±å¤„ç† {len(servers)} ä¸ªèŠ‚ç‚¹")
    auto_deploy_status['steps'][1]['status'] = 'done'
        
    auto_deploy_status['progress'] = 30
    auto_deploy_status['step'] = 3
    auto_deploy_status['steps'][2]['status'] = 'doing'
    auto_deploy_status['log'].append("[3/6] ğŸ“¦ å¼€å§‹ä¸‹è½½å¹¶å®‰è£…Hadoop...")
    
    for i, server in enumerate(servers):
        ssh = ssh_client(server['hostname'], server['username'], server['password'], int(server.get('port', 22)))
        auto_deploy_status['log'].append(format_node_log("[3/6] ğŸ”§ èŠ‚ç‚¹", "æ­£åœ¨å®‰è£…curlå·¥å…·...", server['hostname'], i, len(servers)))
        hadoop_tar_path = "/opt/hadoop.tar.gz"
        stdin, stdout, stderr = ssh.exec_command("yum install -y curl",timeout=60)
        exit_code = stdout.channel.recv_exit_status()
        if exit_code != 0:
            auto_deploy_status['log'].append(format_node_log("[3/6] âŒ èŠ‚ç‚¹", "å®‰è£…curlå¤±è´¥ï¼Œè¯·æ£€æŸ¥yumæºå’Œæƒé™", server['hostname'], i, len(servers)))
            auto_deploy_status['steps'][2]['status'] = 'error'
            return
        
        auto_deploy_status['log'].append(format_node_log("[3/6] ğŸ” èŠ‚ç‚¹", "æ£€æŸ¥Hadoopå®‰è£…åŒ…å®Œæ•´æ€§...", server['hostname'], i, len(servers)))
        stdin, stdout, stderr = ssh.exec_command("sha256sum /opt/hadoop.tar.gz")
        hadoop_tar_hash = stdout.read().decode().strip()
        if hadoop_tar_hash != "f5195059c0d4102adaa7fff17f7b2a85df906bcb6e19948716319f9978641a04  /opt/hadoop.tar.gz":
            auto_deploy_status['log'].append(format_node_log("[3/6] ğŸ“¥ èŠ‚ç‚¹", "å¼€å§‹ä¸‹è½½Hadoop 3.3.6...", server['hostname'], i, len(servers)))
            stdin, stdout, stderr = ssh.exec_command(f"curl -o {hadoop_tar_path} https://mirrors.aliyun.com/apache/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz",timeout=70)
            exit_code = stdout.channel.recv_exit_status()
            if exit_code != 0:
                auto_deploy_status['log'].append(format_node_log("[3/6] âŒ èŠ‚ç‚¹", "ä¸‹è½½Hadoopå¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥", server['hostname'], i, len(servers)))
                auto_deploy_status['steps'][2]['status'] = 'error'
            else:
                auto_deploy_status['log'].append(format_node_log("[3/6] âœ… èŠ‚ç‚¹", "Hadoopä¸‹è½½å®Œæˆ", server['hostname'], i, len(servers)))
        else:
            auto_deploy_status['log'].append(format_node_log("[3/6] âœ… èŠ‚ç‚¹", "Hadoopå®‰è£…åŒ…å·²å­˜åœ¨ä¸”æ ¡éªŒé€šè¿‡", server['hostname'], i, len(servers)))

        auto_deploy_status['log'].append(format_node_log("[3/6] ğŸ“‚ èŠ‚ç‚¹", "å¼€å§‹è§£å‹Hadoop...", server['hostname'], i, len(servers)))
        #ç¡®ä¿æœ‰tarå‘½ä»¤
        ssh.exec_command("yum install -y tar && source /etc/profile")
        stdin, stdout, stderr = ssh.exec_command(f"mkdir -p /opt/hadoop/ && tar -xzf {hadoop_tar_path} -C /opt/")
        exit_code = stdout.channel.recv_exit_status()
        if exit_code != 0:
            auto_deploy_status['log'].append(format_node_log("[3/6] âŒ èŠ‚ç‚¹", "è§£å‹Hadoopå¤±è´¥ï¼Œè¯·æ£€æŸ¥ç£ç›˜ç©ºé—´", server['hostname'], i, len(servers)))
            auto_deploy_status['steps'][2]['status'] = 'error'
            continue
            
        ssh.exec_command(f"mv /opt/hadoop-3.3.6/* /opt/hadoop/ && rm -rf /opt/hadoop-3.3.6")
        auto_deploy_status['log'].append(format_node_log("[3/6] âœ… èŠ‚ç‚¹", "Hadoopå®‰è£…å®Œæˆ", server['hostname'], i, len(servers)))
        ssh.close()
    
    auto_deploy_status['log'].append(f"[3/6] âœ… Hadoopå®‰è£…å®Œæˆï¼å…±å¤„ç† {len(servers)} ä¸ªèŠ‚ç‚¹")
    auto_deploy_status['steps'][2]['status'] = 'done'
    auto_deploy_status['progress'] = 40
    auto_deploy_status['step'] = 4
    auto_deploy_status['steps'][3]['status'] = 'doing'
    auto_deploy_status['log'].append("[4/6] âš™ï¸ å¼€å§‹é…ç½®Hadoopæ ¸å¿ƒå‚æ•°...")

    for i, server in enumerate(servers):
        ssh = ssh_client(server['hostname'], server['username'], server['password'], int(server.get('port', 22)))
        auto_deploy_status['log'].append(format_node_log("[4/6] ğŸ”§ èŠ‚ç‚¹", "å¼€å§‹é…ç½®Hadoopå‚æ•°...", server['hostname'], i, len(servers)))

        auto_deploy_status['log'].append(format_node_log("[4/6] ğŸ“„ èŠ‚ç‚¹", f"é…ç½®core-site.xml (ä¸»èŠ‚ç‚¹: {servers[0]['hostname']})", server['hostname'], i, len(servers)))
        ssh.exec_command(f"echo '{generate_core_site(servers[0]['hostname'])}' > /opt/hadoop/etc/hadoop/core-site.xml")

        auto_deploy_status['log'].append(format_node_log("[4/6] ğŸ“„ èŠ‚ç‚¹", f"é…ç½®hdfs-site.xml (å‰¯æœ¬æ•°: {len(servers)})", server['hostname'], i, len(servers)))
        ssh.exec_command(f"echo '{generate_hdfs_site(len(servers))}' > /opt/hadoop/etc/hadoop/hdfs-site.xml")
        
        auto_deploy_status['log'].append(format_node_log("[4/6] ğŸ“„ èŠ‚ç‚¹", f"é…ç½®yarn-site.xml (èµ„æºç®¡ç†å™¨: {servers[0]['hostname']})", server['hostname'], i, len(servers)))
        ssh.exec_command(f"echo '{generate_yarn_site(servers[0]['hostname'])}' > /opt/hadoop/etc/hadoop/yarn-site.xml")

        auto_deploy_status['log'].append(format_node_log("[4/6] ğŸ“„ èŠ‚ç‚¹", "é…ç½®workersæ–‡ä»¶", server['hostname'], i, len(servers)))
        workers_content = ''.join([server + '\n' for server in generate_workers(servers)])
        ssh.exec_command(f"echo '{workers_content}' > /opt/hadoop/etc/hadoop/workers")

        auto_deploy_status['log'].append(format_node_log("[4/6] ğŸ“„ èŠ‚ç‚¹", "é…ç½®hadoop-env.sh", server['hostname'], i, len(servers)))
        stdin, stdout, stderr = ssh.exec_command(f"source ~/.hadoop_env && dirname $(dirname $(readlink -f $(which java)))")
        java_home = stdout.read().decode().strip()
        ssh.exec_command(f"echo 'export JAVA_HOME={java_home}' > /opt/hadoop/etc/hadoop/hadoop-env.sh")

        auto_deploy_status['log'].append(format_node_log("[4/6] ğŸ”§ èŠ‚ç‚¹", "é…ç½®ç¯å¢ƒå˜é‡", server['hostname'], i, len(servers)))
        env_content = f"""
export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.312.b07-2.el8_5.x86_64
export HADOOP_HOME=/opt/hadoop
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
export HADOOP_OPTS=\"-Djava.library.path=$HADOOP_HOME/lib/native\"
export HDFS_NAMENODE_USER=root
export HDFS_DATANODE_USER=root
export HDFS_SECONDARYNAMENODE_USER=root
export YARN_RESOURCEMANAGER_USER=root
export YARN_NODEMANAGER_USER=root"""
        ssh.exec_command(f"echo '{env_content}' > ~/.hadoop_env && source ~/.hadoop_env")
        auto_deploy_status['log'].append(format_node_log("[4/6] âœ… èŠ‚ç‚¹", "Hadoopé…ç½®å®Œæˆ", server['hostname'], i, len(servers)))
        ssh.close()
    
    auto_deploy_status['log'].append(f"[4/6] âœ… Hadoopé…ç½®å®Œæˆï¼å…±é…ç½® {len(servers)} ä¸ªèŠ‚ç‚¹")
    auto_deploy_status['steps'][3]['status'] = 'done'

    auto_deploy_status['progress'] = 60
    auto_deploy_status['step'] = 5
    auto_deploy_status['steps'][4]['status'] = 'doing'
    auto_deploy_status['log'].append("[5/6] ğŸš€ å¼€å§‹å¯åŠ¨Hadoopé›†ç¾¤æœåŠ¡...")
    
    # åœ¨æ‰€æœ‰èŠ‚ç‚¹ä¸Šæ£€æŸ¥å¹¶åœæ­¢HadoopæœåŠ¡
    auto_deploy_status['log'].append("[5/6] ğŸ” æ£€æŸ¥æ‰€æœ‰èŠ‚ç‚¹ä¸Šçš„HadoopæœåŠ¡çŠ¶æ€...")
    for i, server in enumerate(servers):
        ssh = ssh_client(server['hostname'], server['username'], server['password'], int(server.get('port', 22)))
        
        # æ£€æŸ¥æ˜¯å¦æœ‰Hadoopè¿›ç¨‹åœ¨è¿è¡Œ
        auto_deploy_status['log'].append(format_node_log("[5/6] ğŸ” æ£€æŸ¥èŠ‚ç‚¹", "ä¸Šçš„Hadoopè¿›ç¨‹...", server['hostname'], i, len(servers)))
        stdin, stdout, stderr = ssh.exec_command("source ~/.hadoop_env && ps aux | grep -E '(hadoop|java.*hadoop)' | grep -v grep", timeout=30)
        running_processes = stdout.read().decode().strip()
        
        if running_processes:
            auto_deploy_status['log'].append(format_node_log("[5/6] â¹ï¸ èŠ‚ç‚¹", "å‘ç°è¿è¡Œä¸­çš„Hadoopè¿›ç¨‹ï¼Œå¼€å§‹åœæ­¢...", server['hostname'], i, len(servers)))
            # æ­£å¸¸åœæ­¢æœåŠ¡
            ssh.exec_command("source ~/.hadoop_env && stop-all.sh 2>/dev/null || true", timeout=60)
            time.sleep(2)
            
            # å¼ºåˆ¶ç»ˆæ­¢å‰©ä½™è¿›ç¨‹
            ssh.exec_command("source ~/.hadoop_env && pkill -f hadoop 2>/dev/null || true", timeout=30)
            ssh.exec_command("source ~/.hadoop_env && pkill -f java.*hadoop 2>/dev/null || true", timeout=30)
            time.sleep(1)
            
            # å†æ¬¡æ£€æŸ¥æ˜¯å¦è¿˜æœ‰è¿›ç¨‹
            stdin, stdout, stderr = ssh.exec_command("source ~/.hadoop_env && ps aux | grep -E '(hadoop|java.*hadoop)' | grep -v grep", timeout=30)
            remaining_processes = stdout.read().decode().strip()
            
            if remaining_processes:
                auto_deploy_status['log'].append(format_node_log("[5/6] âš¡ èŠ‚ç‚¹", "ä»æœ‰Hadoopè¿›ç¨‹è¿è¡Œï¼Œå¼ºåˆ¶ç»ˆæ­¢...", server['hostname'], i, len(servers)))
                ssh.exec_command("source ~/.hadoop_env && pkill -9 -f hadoop 2>/dev/null || true", timeout=30)
                ssh.exec_command("source ~/.hadoop_env && pkill -9 -f java.*hadoop 2>/dev/null || true", timeout=30)
            else:
                auto_deploy_status['log'].append(format_node_log("[5/6] âœ… èŠ‚ç‚¹", "çš„HadoopæœåŠ¡å·²æˆåŠŸåœæ­¢", server['hostname'], i, len(servers)))
        else:
            auto_deploy_status['log'].append(format_node_log("[5/6] âœ… èŠ‚ç‚¹", "æ²¡æœ‰è¿è¡Œä¸­çš„Hadoopè¿›ç¨‹", server['hostname'], i, len(servers)))
        
        ssh.close()
    
    time.sleep(3)  # ç­‰å¾…æ‰€æœ‰èŠ‚ç‚¹æœåŠ¡å®Œå…¨åœæ­¢
    
    # åœ¨ä¸»èŠ‚ç‚¹ä¸Šè¿›è¡Œnamenodeæ“ä½œ
    ssh = ssh_client(servers[0]['hostname'], servers[0]['username'], servers[0]['password'], int(servers[0].get('port', 22)))
    
    # æ£€æŸ¥namenodeæ˜¯å¦å·²ç»åˆå§‹åŒ–
    auto_deploy_status['log'].append(f"[5/6] ğŸ” æ£€æŸ¥ä¸»èŠ‚ç‚¹ {servers[0]['hostname']} çš„namenodeçŠ¶æ€...")
    
    # é¦–å…ˆåˆ›å»ºå¿…è¦çš„ç›®å½•
    ssh.exec_command("source ~/.hadoop_env && mkdir -p /opt/hadoop/data/dfs/namenode")
    ssh.exec_command("source ~/.hadoop_env && mkdir -p /opt/hadoop/data/dfs/datanode")

    stdin, stdout, stderr = ssh.exec_command(
        "source ~/.hadoop_env && (test -f /opt/hadoop/data/dfs/namenode/current/VERSION || test -f /data/hadoop/data/dfs/namenode/current/VERSION) && echo 'EXIST' || echo 'NOT_EXIST'",
        timeout=30
    )
    namenode_exists = stdout.read().decode().strip()
    err = stderr.read().decode().strip()
    if err:
        auto_deploy_status['log'].append(f"[è°ƒè¯•] æ£€æŸ¥VERSIONæ–‡ä»¶æ—¶stderr: {err}")
    
    if namenode_exists == "EXIST":
        auto_deploy_status['log'].append(f"[6/7] âœ… ä¸»èŠ‚ç‚¹ {servers[0]['hostname']} namenodeå·²åˆå§‹åŒ–ï¼Œè·³è¿‡æ ¼å¼åŒ–æ­¥éª¤")
    else:
        # è¿›è¡Œæ ¼å¼åŒ–
        auto_deploy_status['log'].append(f"[6/7] ğŸ”§ ä¸»èŠ‚ç‚¹ {servers[0]['hostname']} namenodeæœªåˆå§‹åŒ–ï¼Œå¼€å§‹æ ¼å¼åŒ–...")
        stdin, stdout, stderr = ssh.exec_command("source ~/.hadoop_env && hadoop namenode -format", timeout=120)
        exit_code = stdout.channel.recv_exit_status()
        if exit_code != 0:
            auto_deploy_status['log'].append(f"[6/7] âŒ ä¸»èŠ‚ç‚¹ {servers[0]['hostname']} namenodeæ ¼å¼åŒ–å¤±è´¥")
            auto_deploy_status['steps'][4]['status'] = 'error'
            return
        auto_deploy_status['log'].append(f"[6/7] âœ… ä¸»èŠ‚ç‚¹ {servers[0]['hostname']} namenodeæ ¼å¼åŒ–å®Œæˆ")
    
    # å¯åŠ¨Hadoopé›†ç¾¤æœåŠ¡
    ssh = ssh_client(servers[0]['hostname'], servers[0]['username'], servers[0]['password'], int(servers[0].get('port', 22)))
    auto_deploy_status['log'].append(f"[5/6] ğŸš€ ä»ä¸»èŠ‚ç‚¹ {servers[0]['hostname']} å¯åŠ¨Hadoopé›†ç¾¤æœåŠ¡...")
    stdin,stdout,stderr = ssh.exec_command("source ~/.hadoop_env && /opt/hadoop/sbin/start-all.sh")
    stdout.channel.recv_exit_status()
    auto_deploy_status['log'].append(f"[5/6] âœ… Hadoopé›†ç¾¤æœåŠ¡å¯åŠ¨å®Œæˆ")
    auto_deploy_status['steps'][4]['status'] = 'done'
    auto_deploy_status['progress'] = 80
    auto_deploy_status['step'] = 6
    auto_deploy_status['steps'][5]['status'] = 'doing'
    auto_deploy_status['log'].append("[6/6] ğŸ” å¼€å§‹éªŒè¯Hadoopé›†ç¾¤è¿è¡ŒçŠ¶æ€...")
    auto_deploy_status['log'].append(f"[6/6] ğŸ” æµ‹è¯•HDFSæ–‡ä»¶ç³»ç»Ÿè®¿é—® (ä¸»èŠ‚ç‚¹: {servers[0]['hostname']})...")
    stdin,stdout,stderr = ssh.exec_command("source ~/.hadoop_env && hdfs dfs -ls /",timeout=120)
    ssh.exec_command("source ~/.hadoop_env && hdfs dfs -chmod -R 777 /")
    exit_code = stdout.channel.recv_exit_status()
    if exit_code != 0:
        auto_deploy_status['log'].append(f"[6/6] âŒ HDFSæ–‡ä»¶ç³»ç»ŸéªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥é›†ç¾¤çŠ¶æ€")
        auto_deploy_status['steps'][5]['status'] = 'error'
        return
    
    auto_deploy_status['log'].append(f"[6/6] âœ… HDFSæ–‡ä»¶ç³»ç»ŸéªŒè¯æˆåŠŸ")
    auto_deploy_status['log'].append(f"[6/6] ğŸ‰ Hadoopé›†ç¾¤éƒ¨ç½²å®Œæˆï¼å…± {len(servers)} ä¸ªèŠ‚ç‚¹")
    auto_deploy_status['progress'] = 100
    auto_deploy_status['status'] = 'done'
    auto_deploy_status['steps'][5]['status'] = 'done'

    nn_url = f'http://{servers[0]["hostname"]}:9870'
    rm_url = f'http://{servers[0]["hostname"]}:8088'
    auto_deploy_status['cluster_links'] = {
        'NameNode': nn_url,
        'ResourceManager': rm_url
    }
    auto_deploy_status['log'].append(f"[6/6] ğŸŒ é›†ç¾¤Web UIå·²å°±ç»ªï¼Œä¸»èŠ‚ç‚¹: {servers[0]['hostname']}")



@app.route('/api/upload/java', methods=['POST'])
def upload_java_package():
    try:
        # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡ä»¶ä¸Šä¼ 
        if 'file' not in request.files:
            return jsonify({'success': False, 'error': 'æ²¡æœ‰é€‰æ‹©æ–‡ä»¶'}), 400
        
        file = request.files['file']
        if file.filename == '':
            return jsonify({'success': False, 'error': 'æ²¡æœ‰é€‰æ‹©æ–‡ä»¶'}), 400
        
        # è·å–é…ç½®ä¿¡æ¯
        config = request.form.get('config')
        if not config:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘é…ç½®ä¿¡æ¯'}), 400
        
        if isinstance(config, str):
            config = ast.literal_eval(config)
        
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹ï¼ˆä¿®æ­£ï¼šæ”¯æŒå¤šåç¼€ï¼‰
        allowed_extensions = ('.tar', '.tar.gz', '.tgz', '.zip', '.rpm', '.deb')
        filename = file.filename or 'unknown'
        if not filename.lower().endswith(allowed_extensions):
            return jsonify({'success': False, 'error': f'ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {filename.split(".")[-1]}'}), 400
        
        # ä¿å­˜æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
        import tempfile
        import os
        temp_dir = tempfile.mkdtemp()
        temp_file_path = os.path.join(temp_dir, filename)
        file.save(temp_file_path)
        file_size = os.path.getsize(temp_file_path)
        
        # åªè¿”å›æœ¬åœ°æ–‡ä»¶ä¿¡æ¯ï¼Œä¸åšè¿œç¨‹ä¸Šä¼ 
        return jsonify({
            'success': True,
            'filename': filename,
            'temp_path': temp_file_path,
            'size': file_size,
            'config': config
        })
    except Exception as e:
        return jsonify({'success': False, 'error': f'ä¸Šä¼ è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}'}), 500


@app.route('/api/upload/hadoop', methods=['POST'])
def upload_hadoop_package():
    try:
        # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡ä»¶ä¸Šä¼ 
        if 'file' not in request.files:
            return jsonify({'success': False, 'error': 'æ²¡æœ‰é€‰æ‹©æ–‡ä»¶'}), 400
        
        file = request.files['file']
        if file.filename == '':
            return jsonify({'success': False, 'error': 'æ²¡æœ‰é€‰æ‹©æ–‡ä»¶'}), 400
        
        # è·å–é…ç½®ä¿¡æ¯
        config = request.form.get('config')
        if not config:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘é…ç½®ä¿¡æ¯'}), 400
        
        if isinstance(config, str):
            config = ast.literal_eval(config)
        
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹ï¼ˆä¿®æ­£ï¼šæ”¯æŒå¤šåç¼€ï¼‰
        allowed_extensions = ('.tar', '.tar.gz', '.tgz', '.zip', '.rpm', '.deb')
        filename = file.filename or 'unknown'
        if not filename.lower().endswith(allowed_extensions):
            return jsonify({'success': False, 'error': f'ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {filename.split(".")[-1]}'}), 400
        
        # ä¿å­˜æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
        import tempfile
        import os
        temp_dir = tempfile.mkdtemp()
        temp_file_path = os.path.join(temp_dir, filename)
        file.save(temp_file_path)
        file_size = os.path.getsize(temp_file_path)
        
        # åªè¿”å›æœ¬åœ°æ–‡ä»¶ä¿¡æ¯ï¼Œä¸åšè¿œç¨‹ä¸Šä¼ 
        return jsonify({
            'success': True,
            'filename': filename,
            'temp_path': temp_file_path,
            'size': file_size,
            'config': config
        })
    except Exception as e:
        return jsonify({'success': False, 'error': f'ä¸Šä¼ è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}'}), 500


def semi_auto_deploy_task(config):
    semi_auto_deploy_status['status'] = 'running'
    semi_auto_deploy_status['log'] = []
    semi_auto_deploy_status['steps'] = [
        {'name': 'ç¯å¢ƒæ£€æµ‹', 'status': 'pending'},
        {'name': 'å®‰è£…Javaç¯å¢ƒ', 'status': 'pending'},
        {'name': 'ä¸‹è½½å¹¶è§£å‹Hadoop', 'status': 'pending'},
        {'name': 'è‡ªåŠ¨é…ç½®Hadoopæ ¸å¿ƒå‚æ•°', 'status': 'pending'},
        {'name': 'å®‰è£…ç»„ä»¶', 'status': 'pending'},
        {'name': 'å¯åŠ¨Hadoopé›†ç¾¤æœåŠ¡', 'status': 'pending'},
        {'name': 'éªŒè¯é›†ç¾¤è¿è¡ŒçŠ¶æ€', 'status': 'pending'}
    ]
    if isinstance(config, str):
        config = ast.literal_eval(config)
    # å…³é”®ä¿®æ­£
    servers = config.get('servers', []) if isinstance(config, dict) else config
    basic = config.get('basic', {})
    cluster = config.get('cluster', {})
    components = config.get('components', {})
    advanced = config.get('advanced', {})
    performance = config.get('performance', {})

    semi_auto_deploy_status['step'] = 1
    semi_auto_deploy_status['progress'] = 10
    semi_auto_deploy_status['log'].append("--------------------------------åŠè‡ªåŠ¨éƒ¨ç½²å¼€å§‹--------------------------------")
    semi_auto_deploy_status['log'].append("[1/7] ğŸš€ å¼€å§‹ç¯å¢ƒæ£€æµ‹ä¸SSHå…å¯†é…ç½®...")
    semi_auto_deploy_status['steps'][0]['status'] = 'doing'

    set_hostname(servers)
    pubkey_list = []
    for i, server in enumerate(servers):
        ip = server['hostname']
        username = server['username']
        password = server['password']
        port = int(server.get('port', 22))
        semi_auto_deploy_status['log'].append(format_node_log("[1/7] ğŸ”— æ­£åœ¨è¿æ¥èŠ‚ç‚¹", f"(ç”¨æˆ·: {username}, ç«¯å£: {port})...", ip, i, len(servers)))
        ssh = ssh_client(ip, username, password, port)
        semi_auto_deploy_status['log'].append(format_node_log("[1/7] âœ… èŠ‚ç‚¹", "SSHè¿æ¥æˆåŠŸï¼Œå¼€å§‹é…ç½®SSHå¯†é’¥...", ip, i, len(servers)))
        ssh.exec_command("ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa")
        #ç­‰å¾…ssh-keygenç”Ÿæˆå…¬é’¥
        time.sleep(1)
        #è·å–å…¬é’¥
        stdin, stdout, stderr = ssh.exec_command("cat ~/.ssh/id_rsa.pub")
        pubkey = stdout.read().decode().strip()
        #æ·»åŠ å…¬é’¥åˆ°pubkey_list
        pubkey_list.append(pubkey)
        #æ·»åŠ ipæ˜ å°„åˆ°hosts
        server_count = 0
        for server2 in servers:
            server_count += 1
            if server_count <= 1:
                hostname = 'master'
            elif server_count <= len(servers):
                hostname = f'slave{server_count - 1}'
            ssh.exec_command(f"echo '{server2['hostname']}\t{hostname}' >> /etc/hosts")
            #å…³é—­é˜²ç«å¢™
            close_firewall(ssh)
        semi_auto_deploy_status['log'].append(format_node_log("[1/7] ğŸ”§ èŠ‚ç‚¹", "ä¸»æœºåæ˜ å°„å’Œé˜²ç«å¢™é…ç½®å®Œæˆ", ip, i, len(servers)))
    
    #æ·»åŠ å…¬é’¥åˆ°authorized_keys
    semi_auto_deploy_status['log'].append("[1/7] ğŸ”‘ æ­£åœ¨é…ç½®SSHå…å¯†ç™»å½•...")
    for server in servers:
        ssh = ssh_client(server['hostname'], server['username'], server['password'], int(server.get('port', 22)))
        for pubkey in pubkey_list:
            ssh.exec_command(f"echo '{pubkey}' >> ~/.ssh/authorized_keys")
        ssh.close()

    semi_auto_deploy_status['log'].append(f"[1/7] âœ… ç¯å¢ƒæ£€æµ‹å®Œæˆï¼å…±é…ç½® {len(servers)} ä¸ªèŠ‚ç‚¹")
    semi_auto_deploy_status['steps'][0]['status'] = 'done'

    
    semi_auto_deploy_status['progress'] = 20
    semi_auto_deploy_status['step'] = 2
    semi_auto_deploy_status['steps'][1]['status'] = 'doing'
    semi_auto_deploy_status['log'].append("[2/7] â˜• å¼€å§‹å®‰è£…Javaè¿è¡Œç¯å¢ƒ...")
    
    for i, server in enumerate(servers):
        ssh = ssh_client(server['hostname'], server['username'], server['password'], int(server.get('port', 22)))

        #æ¸…ç©ºé…ç½®æ–‡ä»¶
        ssh.exec_command("rm -rf ~/.hadoop_env")

        semi_auto_deploy_status['log'].append(format_node_log("[2/7] ğŸ” æ£€æŸ¥èŠ‚ç‚¹", "çš„Javaç¯å¢ƒ...", server['hostname'], i, len(servers)))
        stdin, stdout, stderr = ssh.exec_command("which java")
        java_path = stdout.read().decode().strip()
        if not java_path:
            # ä¼˜å…ˆä½¿ç”¨è‡ªå®šä¹‰ä¸Šä¼ çš„JavaåŒ…
            java_temp_path = None
            if isinstance(config, dict):
                java_temp_path = config.get('javaTempPath') or config.get('java_temp_path')
            if java_temp_path and os.path.exists(java_temp_path):
                try:
                    semi_auto_deploy_status['log'].append(format_node_log("[2/7] ğŸ“¦ èŠ‚ç‚¹", f"æ£€æµ‹åˆ°è‡ªå®šä¹‰JavaåŒ…ï¼Œä¸Šä¼ å¹¶è§£å‹: {java_temp_path}", server['hostname'], i, len(servers)))
                    sftp = ssh.open_sftp()
                    remote_path = f"/opt/{os.path.basename(java_temp_path)}"
                    sftp.put(java_temp_path, remote_path)
                    ssh.exec_command("mkdir -p /opt/java")
                    sftp.close()
                    # è§£å‹ï¼ˆæ”¯æŒtar.gz/tgz/zipï¼‰
                    javaHome = "/opt/java"
                    if remote_path.endswith('.zip'):
                        ssh.exec_command(f"unzip -o {remote_path} -d {javaHome} --strip-components=1")
                    else:
                        ssh.exec_command(f"tar -xzf {remote_path} -C {javaHome} --strip-components=1")
                    ssh.exec_command(f"echo 'export JAVA_HOME={javaHome}' >> ~/.hadoop_env")
                    ssh.exec_command(f"echo 'export PATH=$PATH:$JAVA_HOME/bin' >> ~/.hadoop_env")
                    ssh.exec_command(f"echo 'export CLASSPATH=$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/jce.jar' >> ~/.hadoop_env")
                    ssh.exec_command(f"echo 'export JRE_HOME=$JAVA_HOME/jre' >> ~/.hadoop_env")
                    ssh.exec_command(f"echo 'export PATH=$PATH:$JRE_HOME/bin' >> ~/.hadoop_env")
                    ssh.exec_command(f"echo 'export PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin' >> ~/.hadoop_env")
                    ssh.exec_command(f"echo 'export PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin' >> ~/.hadoop_env")
                    ssh.exec_command(f"source ~/.hadoop_env")
                    #åˆ é™¤è‡ªå®šä¹‰åŒ…
                    os.remove(java_temp_path)
                    semi_auto_deploy_status['log'].append(format_node_log("[2/7] âœ… èŠ‚ç‚¹", f"å·²ç”¨è‡ªå®šä¹‰åŒ…å®‰è£…Java: {remote_path}", server['hostname'], i, len(servers)))
                except Exception as e:
                    semi_auto_deploy_status['log'].append(format_node_log("[2/7] âŒ èŠ‚ç‚¹", f"è‡ªå®šä¹‰JavaåŒ…å®‰è£…å¤±è´¥: {e}", server['hostname'], i, len(servers)))
                    semi_auto_deploy_status['steps'][1]['status'] = 'error'
                    return
            else:
                java_version = basic.get('javaVersion')
                semi_auto_deploy_status['log'].append(format_node_log("[2/7] ğŸ“¦ èŠ‚ç‚¹", f"æœªæ£€æµ‹åˆ°Javaï¼Œå¼€å§‹å®‰è£…OpenJDK{java_version}...", server['hostname'], i, len(servers)))
                if java_version == '8':
                    stdin, stdout, stderr = ssh.exec_command("yum install -y java-1.8.0-openjdk-devel")
                    javaHome = "/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.312.b07-2.el8_5.x86_64"
                elif java_version == '11':
                    stdin, stdout, stderr = ssh.exec_command("yum install -y java-11-openjdk-devel")
                    javaHome = "/usr/lib/jvm/java-11-openjdk-11.0.13.0.8-4.el8_8.x86_64"
                elif java_version == '17':
                    stdin, stdout, stderr = ssh.exec_command("yum install -y java-17-openjdk-devel")
                    javaHome = "/usr/lib/jvm/java-17-openjdk-17.0.11.0.8-2.el8_8.x86_64"
                semi_auto_deploy_status['log'].append(format_node_log("[2/7] â³ èŠ‚ç‚¹", f"æ­£åœ¨å®‰è£…Java{java_version}ï¼Œè¯·ç¨å€™...", server['hostname'], i, len(servers)))
                exit_code = stdout.channel.recv_exit_status()
                if exit_code != 0:
                    semi_auto_deploy_status['log'].append(format_node_log("[2/7] âŒ èŠ‚ç‚¹", f"å®‰è£…Java{java_version}å¤±è´¥ï¼Œè¯·æ£€æŸ¥yumæºå’Œæƒé™", server['hostname'], i, len(servers)))
                    semi_auto_deploy_status['steps'][1]['status'] = 'error'
                    return
                semi_auto_deploy_status['log'].append(format_node_log("[2/7] âœ… èŠ‚ç‚¹", f"Java{java_version}å®‰è£…æˆåŠŸ", server['hostname'], i, len(servers)))
        else:
            semi_auto_deploy_status['log'].append(format_node_log("[2/7] âœ… èŠ‚ç‚¹", f"å·²å­˜åœ¨Javaç¯å¢ƒ: {java_path}", server['hostname'], i, len(servers)))

    stdin, stdout, stderr = ssh.exec_command(f"dirname $(dirname $(readlink -f {java_path}))")
    javaHome = stdout.read().decode().strip()

    semi_auto_deploy_status['log'].append(f"[2/7] âœ… Javaç¯å¢ƒå®‰è£…å®Œæˆï¼å…±å¤„ç† {len(servers)} ä¸ªèŠ‚ç‚¹")
    semi_auto_deploy_status['steps'][1]['status'] = 'done'
        
    semi_auto_deploy_status['progress'] = 30
    semi_auto_deploy_status['step'] = 3
    semi_auto_deploy_status['steps'][2]['status'] = 'doing'
    semi_auto_deploy_status['log'].append("[3/7] ğŸ“¦ å¼€å§‹ä¸‹è½½å¹¶å®‰è£…Hadoop...")
    
    for i, server in enumerate(servers):
        ssh = ssh_client(server['hostname'], server['username'], server['password'], int(server.get('port', 22)))
        # ä¼˜å…ˆä½¿ç”¨è‡ªå®šä¹‰ä¸Šä¼ çš„HadoopåŒ…
        hadoop_temp_path = None
        if isinstance(config, dict):
            hadoop_temp_path = config.get('hadoopTempPath') or config.get('hadoop_temp_path')
        if hadoop_temp_path and os.path.exists(hadoop_temp_path):
            try:
                semi_auto_deploy_status['log'].append(format_node_log("[3/7] ğŸ“¦ èŠ‚ç‚¹", f"æ£€æµ‹åˆ°è‡ªå®šä¹‰HadoopåŒ…ï¼Œä¸Šä¼ å¹¶è§£å‹: {hadoop_temp_path}", server['hostname'], i, len(servers)))
                sftp = ssh.open_sftp()
                remote_path = f"/opt/{os.path.basename(hadoop_temp_path)}"
                sftp.put(hadoop_temp_path, remote_path)
                sftp.close()
                ssh.exec_command("yum install -y tar && source /etc/profile")
                ssh.exec_command(f"rm -rf /opt/hadoop && mkdir -p /opt/hadoop")
                stdin, stdout, stderr = ssh.exec_command(f"tar -xzf {remote_path} -C /opt/hadoop --strip-components=1")
                exit_code = stdout.channel.recv_exit_status()
                if exit_code != 0:
                    semi_auto_deploy_status['log'].append(format_node_log("[3/6] âŒ èŠ‚ç‚¹", "è§£å‹Hadoopå¤±è´¥ï¼Œè¯·æ£€æŸ¥ç£ç›˜ç©ºé—´", server['hostname'], i, len(servers)))
                    semi_auto_deploy_status['steps'][2]['status'] = 'error'
                    ssh.close()
                    continue
                #åˆ é™¤è‡ªå®šä¹‰åŒ…
                os.remove(hadoop_temp_path)
                semi_auto_deploy_status['log'].append(format_node_log("[3/7] âœ… èŠ‚ç‚¹", f"å·²ç”¨è‡ªå®šä¹‰åŒ…å®‰è£…Hadoop: {remote_path}", server['hostname'], i, len(servers)))
            except Exception as e:
                semi_auto_deploy_status['log'].append(format_node_log("[3/7] âŒ èŠ‚ç‚¹", f"è‡ªå®šä¹‰HadoopåŒ…å®‰è£…å¤±è´¥: {e}", server['hostname'], i, len(servers)))
                semi_auto_deploy_status['steps'][2]['status'] = 'error'
                ssh.close()
                continue
        else:
            hadoop_version = basic.get('hadoopVersion')
            hadoopHome = basic.get('hadoopHome')
            semi_auto_deploy_status['log'].append(format_node_log("[3/7] ğŸ” èŠ‚ç‚¹", "æ£€æŸ¥Hadoopå®‰è£…åŒ…å®Œæ•´æ€§...", server['hostname'], i, len(servers)))
            stdin, stdout, stderr = ssh.exec_command(f"sha256sum {hadoopHome}.tar.gz")
            hadoop_tar_hash = stdout.read().decode().strip()
            if hadoop_tar_hash != f"f5195059c0d4102adaa7fff17f7b2a85df906bcb6e19948716319f9978641a04  {hadoopHome}.tar.gz":
                semi_auto_deploy_status['log'].append(format_node_log("[3/7] âŒ èŠ‚ç‚¹", "Hadoopå®‰è£…åŒ…æ ¡éªŒå¤±è´¥", server['hostname'], i, len(servers)))
                ssh.exec_command(f"rm -rf {hadoopHome}.tar.gz")
                semi_auto_deploy_status['log'].append(format_node_log("[3/7] ğŸ“¥ èŠ‚ç‚¹", f"å¼€å§‹ä¸‹è½½Hadoop {hadoop_version}...", server['hostname'], i, len(servers)))
                stdin, stdout, stderr = ssh.exec_command(f"curl -o {hadoopHome}.tar.gz https://mirrors.aliyun.com/apache/hadoop/common/hadoop-{hadoop_version}/hadoop-{hadoop_version}.tar.gz",timeout=70)
                exit_code = stdout.channel.recv_exit_status()
                if exit_code != 0:
                    semi_auto_deploy_status['log'].append(format_node_log("[3/7] âŒ èŠ‚ç‚¹", "ä¸‹è½½Hadoopå¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥", server['hostname'], i, len(servers)))
                    semi_auto_deploy_status['steps'][2]['status'] = 'error'
                else:
                    semi_auto_deploy_status['log'].append(format_node_log("[3/7] âœ… èŠ‚ç‚¹", "Hadoopä¸‹è½½å®Œæˆ", server['hostname'], i, len(servers)))
            else:
                ssh.exec_command(f"rm -rf {hadoopHome}")
                semi_auto_deploy_status['log'].append(format_node_log("[3/7] âœ… èŠ‚ç‚¹", f"Hadoopå®‰è£…åŒ…{hadoop_version}å·²å­˜åœ¨ä¸”æ ¡éªŒé€šè¿‡", server['hostname'], i, len(servers)))
            semi_auto_deploy_status['log'].append(format_node_log("[3/7] ğŸ“‚ èŠ‚ç‚¹", "å¼€å§‹è§£å‹Hadoop...", server['hostname'], i, len(servers)))
            ssh.exec_command("yum install -y tar && source /etc/profile")
            stdin, stdout, stderr = ssh.exec_command(f"mkdir -p {hadoopHome} && tar -xzf {hadoopHome}.tar.gz -C {hadoopHome} --strip-components=1")
            exit_code = stdout.channel.recv_exit_status()
            if exit_code != 0:
                semi_auto_deploy_status['log'].append(format_node_log("[3/7] âŒ èŠ‚ç‚¹", "è§£å‹Hadoopå¤±è´¥ï¼Œè¯·æ£€æŸ¥ç£ç›˜ç©ºé—´", server['hostname'], i, len(servers)))
                semi_auto_deploy_status['steps'][2]['status'] = 'error'
                ssh.close()
                continue
            # ssh.exec_command(f"mv /opt/hadoop-3.3.6/* /opt/hadoop/ && rm -rf /opt/hadoop-3.3.6")
            auto_deploy_status['log'].append(format_node_log("[3/7] âœ… èŠ‚ç‚¹", "Hadoopå®‰è£…å®Œæˆ", server['hostname'], i, len(servers)))
        ssh.close()
    
    semi_auto_deploy_status['log'].append(f"[3/7] âœ… Hadoopå®‰è£…å®Œæˆï¼å…±å¤„ç† {len(servers)} ä¸ªèŠ‚ç‚¹")
    semi_auto_deploy_status['steps'][2]['status'] = 'done'
    semi_auto_deploy_status['progress'] = 40
    semi_auto_deploy_status['step'] = 4
    semi_auto_deploy_status['steps'][3]['status'] = 'doing'
    semi_auto_deploy_status['log'].append("[4/7] âš™ï¸ å¼€å§‹é…ç½®Hadoopæ ¸å¿ƒå‚æ•°...")

    for i, server in enumerate(servers):
        ssh = ssh_client(server['hostname'], server['username'], server['password'], int(server.get('port', 22)))
        semi_auto_deploy_status['log'].append(format_node_log("[4/7] ğŸ”§ èŠ‚ç‚¹", "å¼€å§‹é…ç½®Hadoopå‚æ•°...", server['hostname'], i, len(servers)))

        namenodeHost = basic.get('namenodeHost')
        dataDir = basic.get('dataDir')
        ssh.exec_command(f"rm -rf {dataDir} && mkdir -p {dataDir}")
        semi_auto_deploy_status['log'].append(format_node_log("[4/7] ğŸ“„ èŠ‚ç‚¹", f"é…ç½®core-site.xml (ä¸»èŠ‚ç‚¹: {namenodeHost})", server['hostname'], i, len(servers)))
        ssh.exec_command(f"echo '{generate_core_site(namenodeHost, dataDir)}' > {hadoopHome}/etc/hadoop/core-site.xml")

        replicationFactor = cluster.get('replicationFactor')
        namenodeDir = dataDir + "/data/dfs/namenode"
        datanodeDir = dataDir + "/data/dfs/datanode"
        semi_auto_deploy_status['log'].append(format_node_log("[4/7] ğŸ“„ èŠ‚ç‚¹", f"é…ç½®hdfs-site.xml (å‰¯æœ¬æ•°: {replicationFactor})", server['hostname'], i, len(servers)))
        ssh.exec_command(f"echo '{generate_hdfs_site(replicationFactor, namenodeDir, datanodeDir)}' > {hadoopHome}/etc/hadoop/hdfs-site.xml")
        
        mapReduceMemory = cluster.get('mapReduceMemory')
        mapReduceCores = cluster.get('mapReduceCores')
        semi_auto_deploy_status['log'].append(format_node_log("[4/7] ğŸ“„ èŠ‚ç‚¹", f"é…ç½®mapred-site.xml (MapReduceå†…å­˜: {mapReduceMemory}MB, æ ¸å¿ƒæ•°: {mapReduceCores})", server['hostname'], i, len(servers)))
        ssh.exec_command(f"echo '{generate_mapred_site(mapReduceMemory, mapReduceCores)}' > {hadoopHome}/etc/hadoop/mapred-site.xml")

        yarnMemory = cluster.get('yarnMemory')
        yarnCores = cluster.get('yarnCores')
        semi_auto_deploy_status['log'].append(format_node_log("[4/7] ğŸ“„ èŠ‚ç‚¹", f"é…ç½®yarn-site.xml (èµ„æºç®¡ç†å™¨: {yarnMemory}MB, æ ¸å¿ƒæ•°: {yarnCores})", server['hostname'], i, len(servers)))
        ssh.exec_command(f"echo '{generate_yarn_site(namenodeHost, yarnMemory, yarnCores)}' > {hadoopHome}/etc/hadoop/yarn-site.xml")

        datanodeCount = cluster.get('datanodeCount')
        datanodeCount = int(datanodeCount)
        semi_auto_deploy_status['log'].append(format_node_log("[4/7] ğŸ“„ èŠ‚ç‚¹", f"é…ç½®workersæ–‡ä»¶ (DataNodeæ•°é‡: {datanodeCount})", server['hostname'], i, len(servers)))
        workers_content = ''.join([server + '\n' for server in generate_workers(servers, datanodeCount)])
        ssh.exec_command(f"echo '{workers_content}' > {hadoopHome}/etc/hadoop/workers")

        semi_auto_deploy_status['log'].append(format_node_log("[4/7] ğŸ“„ èŠ‚ç‚¹", "é…ç½®hadoop-env.sh", server['hostname'], i, len(servers)))
        ssh.exec_command(f"echo 'export JAVA_HOME={javaHome}' > {hadoopHome}/etc/hadoop/hadoop-env.sh")

        semi_auto_deploy_status['log'].append(format_node_log("[4/7] ğŸ”§ èŠ‚ç‚¹", "é…ç½®ç¯å¢ƒå˜é‡", server['hostname'], i, len(servers)))
        env_content = f"""
export JAVA_HOME={javaHome}
export HADOOP_HOME={hadoopHome}
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
export HADOOP_OPTS=\"-Djava.library.path=$HADOOP_HOME/lib/native\"
export HDFS_NAMENODE_USER=root
export HDFS_DATANODE_USER=root
export HDFS_SECONDARYNAMENODE_USER=root
export YARN_RESOURCEMANAGER_USER=root
export YARN_NODEMANAGER_USER=root"""
        ssh.exec_command(f"echo '{env_content}' > ~/.hadoop_env && source ~/.hadoop_env")
        auto_deploy_status['log'].append(format_node_log("[4/7] âœ… èŠ‚ç‚¹", "Hadoopé…ç½®å®Œæˆ", server['hostname'], i, len(servers)))
        ssh.close()
    
    semi_auto_deploy_status['log'].append(f"[4/6] âœ… Hadoopé…ç½®å®Œæˆï¼å…±é…ç½® {datanodeCount} ä¸ªDataNode")
    semi_auto_deploy_status['steps'][3]['status'] = 'done'

    semi_auto_deploy_status['progress'] = 50
    semi_auto_deploy_status['step'] = 5
    semi_auto_deploy_status['steps'][4]['status'] = 'doing'
    installHive = components.get('installHive')
    installHBase = components.get('installHBase')
    installSpark = components.get('installSpark')
    installZooKeeper = components.get('installZooKeeper')
    installKafka = components.get('installKafka')
    installPig = components.get('installPig')
    
    installItems = []
    if installHive:
        installItems.append('Hive')
        semi_auto_deploy_status['log'].append(f"[5/7] ğŸ” èŠ‚ç‚¹ {servers[0]['hostname']} å¼€å§‹å®‰è£…Hive...")
        ssh = ssh_client(servers[0]['hostname'], servers[0]['username'], servers[0]['password'], int(servers[0].get('port', 22)))
        stdin, stdout, stderr = ssh.exec_command(f"curl -o /opt/hive.tar.gz https://mirrors.aliyun.com/apache/hive/hive-4.0.1/apache-hive-4.0.1-bin.tar.gz")
        exit_code = stdout.channel.recv_exit_status()
        if exit_code != 0:
            semi_auto_deploy_status['log'].append(format_node_log("[5/7] âŒ èŠ‚ç‚¹", "Hiveä¸‹è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œ", server['hostname'], i, len(servers)))
            semi_auto_deploy_status['steps'][4]['status'] = 'error'
            return
        ssh.exec_command(f"mkdir -p /opt/hive && tar -xzf /opt/hive.tar.gz -C /opt/hive --strip-components=1")
        ssh.exec_command(f"echo 'export HIVE_HOME=/opt/hive' >> ~/.hadoop_env")
        ssh.exec_command(f"echo 'export PATH=$PATH:$HIVE_HOME/bin' >> ~/.hadoop_env")
        ssh.exec_command(f"source ~/.hadoop_env")
        ssh.close()
        semi_auto_deploy_status['log'].append(f"[5/7] âœ… èŠ‚ç‚¹ {servers[0]['hostname']} Hiveå®‰è£…å®Œæˆ")

    if installHBase:
        installItems.append('HBase')
        semi_auto_deploy_status['log'].append(f"[5/7] ğŸ” èŠ‚ç‚¹ {servers[0]['hostname']} å¼€å§‹å®‰è£…HBase...")
        ssh = ssh_client(servers[0]['hostname'], servers[0]['username'], servers[0]['password'], int(servers[0].get('port', 22)))
        stdin, stdout, stderr = ssh.exec_command(f"curl -o /opt/hbase.tar.gz https://mirrors.aliyun.com/apache/hbase/2.6.2/hbase-2.6.2-hadoop3-bin.tar.gz")
        exit_code = stdout.channel.recv_exit_status()
        if exit_code != 0:
            semi_auto_deploy_status['log'].append(format_node_log("[5/7] âŒ èŠ‚ç‚¹", "HBaseä¸‹è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œ", server['hostname'], i, len(servers)))
            semi_auto_deploy_status['steps'][4]['status'] = 'error'
            return
        ssh.exec_command(f"mkdir -p /opt/hbase && tar -xzf /opt/hbase.tar.gz -C /opt/hbase --strip-components=1")
        ssh.exec_command(f"echo 'export HBASE_HOME=/opt/hbase' >> ~/.hadoop_env")
        ssh.exec_command(f"echo 'export PATH=$PATH:$HBASE_HOME/bin' >> ~/.hadoop_env")
        ssh.exec_command(f"source ~/.hadoop_env")
        ssh.close()
        semi_auto_deploy_status['log'].append(f"[5/7] âœ… èŠ‚ç‚¹ {servers[0]['hostname']} HBaseå®‰è£…å®Œæˆ")

    if installSpark:
        installItems.append('Spark')
        semi_auto_deploy_status['log'].append(f"[5/7] ğŸ” èŠ‚ç‚¹ {servers[0]['hostname']} å¼€å§‹å®‰è£…Spark...")
        ssh = ssh_client(servers[0]['hostname'], servers[0]['username'], servers[0]['password'], int(servers[0].get('port', 22)))
        stdin, stdout, stderr = ssh.exec_command(f"curl -o /opt/spark.tar.gz https://mirrors.aliyun.com/apache/spark/spark-4.0.0/spark-4.0.0-bin-hadoop3.tgz")
        exit_code = stdout.channel.recv_exit_status()
        if exit_code != 0:
            semi_auto_deploy_status['log'].append(format_node_log("[5/7] âŒ èŠ‚ç‚¹", "Sparkä¸‹è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œ", server['hostname'], i, len(servers)))
            semi_auto_deploy_status['steps'][4]['status'] = 'error'
            return
        ssh.exec_command(f"mkdir -p /opt/spark && tar -xzf /opt/spark.tar.gz -C /opt/spark --strip-components=1")
        ssh.exec_command(f"echo 'export SPARK_HOME=/opt/spark' >> ~/.hadoop_env")
        ssh.exec_command(f"echo 'export PATH=$PATH:$SPARK_HOME/bin' >> ~/.hadoop_env")
        ssh.exec_command(f"source ~/.hadoop_env")
        ssh.close()
        semi_auto_deploy_status['log'].append(f"[5/7] âœ… èŠ‚ç‚¹ {servers[0]['hostname']} Sparkå®‰è£…å®Œæˆ")

    if installZooKeeper:
        installItems.append('ZooKeeper')
        semi_auto_deploy_status['log'].append(f"[5/7] ğŸ” èŠ‚ç‚¹ {servers[0]['hostname']} å¼€å§‹å®‰è£…ZooKeeper...")
        ssh = ssh_client(servers[0]['hostname'], servers[0]['username'], servers[0]['password'], int(servers[0].get('port', 22)))
        stdin, stdout, stderr = ssh.exec_command(f"curl -o /opt/zookeeper.tar.gz https://mirrors.aliyun.com/apache/zookeeper/zookeeper-3.7.2/apache-zookeeper-3.7.2-bin.tar.gz")
        exit_code = stdout.channel.recv_exit_status()
        if exit_code != 0:
            semi_auto_deploy_status['log'].append(format_node_log("[5/7] âŒ èŠ‚ç‚¹", "ZooKeeperä¸‹è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œ", server['hostname'], i, len(servers)))
            semi_auto_deploy_status['steps'][4]['status'] = 'error'
            return
        ssh.exec_command(f"mkdir -p /opt/zookeeper && tar -xzf /opt/zookeeper.tar.gz -C /opt/zookeeper --strip-components=1")
        ssh.exec_command(f"echo 'export ZOOKEEPER_HOME=/opt/zookeeper' >> ~/.hadoop_env")
        ssh.exec_command(f"echo 'export PATH=$PATH:$ZOOKEEPER_HOME/bin' >> ~/.hadoop_env")
        ssh.exec_command(f"source ~/.hadoop_env")
        ssh.close()
        semi_auto_deploy_status['log'].append(f"[5/7] âœ… èŠ‚ç‚¹ {servers[0]['hostname']} ZooKeeperå®‰è£…å®Œæˆ")

    if installKafka:
        installItems.append('Kafka')
        semi_auto_deploy_status['log'].append(f"[5/7] ğŸ” èŠ‚ç‚¹ {servers[0]['hostname']} å¼€å§‹å®‰è£…Kafka...")
        ssh = ssh_client(servers[0]['hostname'], servers[0]['username'], servers[0]['password'], int(servers[0].get('port', 22)))
        stdin, stdout, stderr = ssh.exec_command(f"curl -o /opt/kafka.tar.gz https://mirrors.aliyun.com/apache/kafka/3.9.0/kafka_2.13-3.9.0.tgz")
        exit_code = stdout.channel.recv_exit_status()
        if exit_code != 0:
            semi_auto_deploy_status['log'].append(format_node_log("[5/7] âŒ èŠ‚ç‚¹", "Kafkaä¸‹è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œ", server['hostname'], i, len(servers)))
            semi_auto_deploy_status['steps'][4]['status'] = 'error'
            return
        ssh.exec_command(f"mkdir -p /opt/kafka && tar -xzf /opt/kafka.tar.gz -C /opt/kafka --strip-components=1")
        ssh.exec_command(f"echo 'export KAFKA_HOME=/opt/kafka' >> ~/.hadoop_env")
        ssh.exec_command(f"echo 'export PATH=$PATH:$KAFKA_HOME/bin' >> ~/.hadoop_env")
        ssh.exec_command(f"source ~/.hadoop_env")
        ssh.close()
        semi_auto_deploy_status['log'].append(f"[5/7] âœ… èŠ‚ç‚¹ {servers[0]['hostname']} Kafkaå®‰è£…å®Œæˆ")
        
    if installPig:
        installItems.append('Pig')
        semi_auto_deploy_status['log'].append(f"[5/7] ğŸ” èŠ‚ç‚¹ {servers[0]['hostname']} å¼€å§‹å®‰è£…Pig...")
        ssh = ssh_client(servers[0]['hostname'], servers[0]['username'], servers[0]['password'], int(servers[0].get('port', 22)))
        stdin, stdout, stderr = ssh.exec_command(f"curl -o /opt/pig.tar.gz https://mirrors.aliyun.com/apache/pig/pig-0.17.0/pig-0.17.0.tar.gz")
        exit_code = stdout.channel.recv_exit_status()
        if exit_code != 0:
            semi_auto_deploy_status['log'].append(format_node_log("[5/7] âŒ èŠ‚ç‚¹", "Pigä¸‹è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œ", server['hostname'], i, len(servers)))
            semi_auto_deploy_status['steps'][4]['status'] = 'error'
            return
        ssh.exec_command(f"mkdir -p /opt/pig && tar -xzf /opt/pig.tar.gz -C /opt/pig --strip-components=1")
        ssh.exec_command(f"echo 'export PIG_HOME=/opt/pig' >> ~/.hadoop_env")
        ssh.exec_command(f"echo 'export PATH=$PATH:$PIG_HOME/bin' >> ~/.hadoop_env")
        ssh.exec_command(f"source ~/.hadoop_env")
        ssh.close()
        semi_auto_deploy_status['log'].append(f"[5/7] âœ… èŠ‚ç‚¹ {servers[0]['hostname']} Pigå®‰è£…å®Œæˆ")

    if installItems:
        semi_auto_deploy_status['log'].append(f"[5/7] âœ… èŠ‚ç‚¹ {servers[0]['hostname']} å®‰è£…{', '.join(installItems)}å®Œæˆ")
    else:  
        semi_auto_deploy_status['log'].append(f"[5/7] âœ… èŠ‚ç‚¹ {servers[0]['hostname']} æœªå®‰è£…ä»»ä½•ç»„ä»¶")
        
    semi_auto_deploy_status['steps'][4]['status'] = 'done'

    semi_auto_deploy_status['progress'] = 60
    semi_auto_deploy_status['step'] = 6
    semi_auto_deploy_status['steps'][5]['status'] = 'doing'
    semi_auto_deploy_status['log'].append("[6/7] ğŸš€ å¼€å§‹å¯åŠ¨Hadoopé›†ç¾¤æœåŠ¡...")
    
    # åœ¨æ‰€æœ‰èŠ‚ç‚¹ä¸Šæ£€æŸ¥å¹¶åœæ­¢HadoopæœåŠ¡
    semi_auto_deploy_status['log'].append("[6/7] ğŸ” æ£€æŸ¥æ‰€æœ‰èŠ‚ç‚¹ä¸Šçš„HadoopæœåŠ¡çŠ¶æ€...")
    for i, server in enumerate(servers):
        ssh = ssh_client(server['hostname'], server['username'], server['password'], int(server.get('port', 22)))
        
        # æ£€æŸ¥æ˜¯å¦æœ‰Hadoopè¿›ç¨‹åœ¨è¿è¡Œ
        semi_auto_deploy_status['log'].append(format_node_log("[6/7] ğŸ” æ£€æŸ¥èŠ‚ç‚¹", "ä¸Šçš„Hadoopè¿›ç¨‹...", server['hostname'], i, len(servers)))
        stdin, stdout, stderr = ssh.exec_command("source ~/.hadoop_env && ps aux | grep -E '(hadoop|java.*hadoop)' | grep -v grep", timeout=30)
        running_processes = stdout.read().decode().strip()
        
        if running_processes:
            semi_auto_deploy_status['log'].append(format_node_log("[6/7] â¹ï¸ èŠ‚ç‚¹", "å‘ç°è¿è¡Œä¸­çš„Hadoopè¿›ç¨‹ï¼Œå¼€å§‹åœæ­¢...", server['hostname'], i, len(servers)))
            # æ­£å¸¸åœæ­¢æœåŠ¡
            stdin,stdout,stderr = ssh.exec_command(f"source ~/.hadoop_env && {hadoopHome}/sbin/stop-all.sh", timeout=60)
            stdout.channel.recv_exit_status()
            time.sleep(2)
            
            # å¼ºåˆ¶ç»ˆæ­¢å‰©ä½™è¿›ç¨‹
            ssh.exec_command("source ~/.hadoop_env && pkill -f hadoop 2>/dev/null || true", timeout=30)
            ssh.exec_command("source ~/.hadoop_env && pkill -f java.*hadoop 2>/dev/null || true", timeout=30)
            time.sleep(1)
            
            # å†æ¬¡æ£€æŸ¥æ˜¯å¦è¿˜æœ‰è¿›ç¨‹
            stdin, stdout, stderr = ssh.exec_command("source ~/.hadoop_env && ps aux | grep -E '(hadoop|java.*hadoop)' | grep -v grep", timeout=30)
            remaining_processes = stdout.read().decode().strip()
            
            if remaining_processes:
                semi_auto_deploy_status['log'].append(format_node_log("[6/7] âš¡ èŠ‚ç‚¹", "ä»æœ‰Hadoopè¿›ç¨‹è¿è¡Œï¼Œå¼ºåˆ¶ç»ˆæ­¢...", server['hostname'], i, len(servers)))
                ssh.exec_command("source ~/.hadoop_env && pkill -9 -f hadoop 2>/dev/null || true", timeout=30)
                ssh.exec_command("source ~/.hadoop_env && pkill -9 -f java.*hadoop 2>/dev/null || true", timeout=30)
            else:
                semi_auto_deploy_status['log'].append(format_node_log("[6/7] âœ… èŠ‚ç‚¹", "çš„HadoopæœåŠ¡å·²æˆåŠŸåœæ­¢", server['hostname'], i, len(servers)))
        else:
            semi_auto_deploy_status['log'].append(format_node_log("[6/7] âœ… èŠ‚ç‚¹", "æ²¡æœ‰è¿è¡Œä¸­çš„Hadoopè¿›ç¨‹", server['hostname'], i, len(servers)))
        
        ssh.close()
    
    time.sleep(3)  # ç­‰å¾…æ‰€æœ‰èŠ‚ç‚¹æœåŠ¡å®Œå…¨åœæ­¢
    
    # åœ¨ä¸»èŠ‚ç‚¹ä¸Šè¿›è¡Œnamenodeæ“ä½œ
    ssh = ssh_client(servers[0]['hostname'], servers[0]['username'], servers[0]['password'], int(servers[0].get('port', 22)))
    
    # æ£€æŸ¥namenodeæ˜¯å¦å·²ç»åˆå§‹åŒ–
    semi_auto_deploy_status['log'].append(f"[6/7] ğŸ” æ£€æŸ¥ä¸»èŠ‚ç‚¹ {servers[0]['hostname']} çš„namenodeçŠ¶æ€...")
    
    # é¦–å…ˆåˆ›å»ºå¿…è¦çš„ç›®å½•
    ssh.exec_command(f"source ~/.hadoop_env && mkdir -p {hadoopHome}/data/dfs/namenode")
    ssh.exec_command(f"source ~/.hadoop_env && mkdir -p {hadoopHome}/data/dfs/datanode")
    
    # æ£€æŸ¥VERSIONæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    stdin, stdout, stderr = ssh.exec_command(
        f"source ~/.hadoop_env && (test -f {hadoopHome}/data/dfs/namenode/current/VERSION && echo 'EXIST' || echo 'NOT_EXIST'",
        timeout=30
    )
    namenode_exists = stdout.read().decode().strip()
    err = stderr.read().decode().strip()
    if err:
        semi_auto_deploy_status['log'].append(f"[è°ƒè¯•] æ£€æŸ¥VERSIONæ–‡ä»¶æ—¶é”™è¯¯: {err}")

    if namenode_exists == "EXIST":
        semi_auto_deploy_status['log'].append(f"[6/7] âœ… ä¸»èŠ‚ç‚¹ {servers[0]['hostname']} namenodeå·²åˆå§‹åŒ–ï¼Œè·³è¿‡æ ¼å¼åŒ–æ­¥éª¤")
    else:
        # è¿›è¡Œæ ¼å¼åŒ–
        semi_auto_deploy_status['log'].append(f"[6/7] ğŸ”§ ä¸»èŠ‚ç‚¹ {servers[0]['hostname']} namenodeæœªåˆå§‹åŒ–ï¼Œå¼€å§‹æ ¼å¼åŒ–...")
        stdin, stdout, stderr = ssh.exec_command("source ~/.hadoop_env && hadoop namenode -format", timeout=120)
        err_msg = stderr.read().decode().strip()
        # ç­‰å¾…namenodeåˆå§‹åŒ–å®Œæˆ
        exit_code = stdout.channel.recv_exit_status()
        if exit_code != 0:
            semi_auto_deploy_status['log'].append(f"[6/7] âŒ ä¸»èŠ‚ç‚¹ {servers[0]['hostname']} namenodeæ ¼å¼åŒ–å¤±è´¥: {err_msg}")
            semi_auto_deploy_status['steps'][5]['status'] = 'error'
            return
        semi_auto_deploy_status['log'].append(f"[6/7] âœ… ä¸»èŠ‚ç‚¹ {servers[0]['hostname']} namenodeæ ¼å¼åŒ–å®Œæˆ")
    
    # å¯åŠ¨Hadoopé›†ç¾¤æœåŠ¡
    ssh = ssh_client(servers[0]['hostname'], servers[0]['username'], servers[0]['password'], int(servers[0].get('port', 22)))
    semi_auto_deploy_status['log'].append(f"[6/7] ğŸš€ ä»ä¸»èŠ‚ç‚¹ {servers[0]['hostname']} å¯åŠ¨Hadoopé›†ç¾¤æœåŠ¡...")
    stdin,stdout,stderr = ssh.exec_command("source ~/.hadoop_env && start-all.sh")
    stdout.channel.recv_exit_status()
    semi_auto_deploy_status['log'].append(f"[6/7] âœ… Hadoopé›†ç¾¤æœåŠ¡å¯åŠ¨å®Œæˆ")
    semi_auto_deploy_status['steps'][5]['status'] = 'done'
    semi_auto_deploy_status['progress'] = 80
    semi_auto_deploy_status['step'] = 7
    semi_auto_deploy_status['steps'][6]['status'] = 'doing'
    semi_auto_deploy_status['log'].append("[7/7] ğŸ” å¼€å§‹éªŒè¯Hadoopé›†ç¾¤è¿è¡ŒçŠ¶æ€...")
    semi_auto_deploy_status['log'].append(f"[7/7] ğŸ” æµ‹è¯•HDFSæ–‡ä»¶ç³»ç»Ÿè®¿é—® (ä¸»èŠ‚ç‚¹: {servers[0]['hostname']})...")
    ssh.exec_command(f"source ~/.hadoop_env && hdfs dfs -chmod -R 777 /")
    stdin,stdout,stderr = ssh.exec_command("source ~/.hadoop_env && hdfs dfs -ls /",timeout=120)
    exit_code = stdout.channel.recv_exit_status()
    if exit_code != 0:
        semi_auto_deploy_status['log'].append(f"[7/7] âŒ HDFSæ–‡ä»¶ç³»ç»ŸéªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥é›†ç¾¤çŠ¶æ€")
        semi_auto_deploy_status['steps'][6]['status'] = 'error'
        return
    
    semi_auto_deploy_status['log'].append(f"[7/7] âœ… HDFSæ–‡ä»¶ç³»ç»ŸéªŒè¯æˆåŠŸ")
    semi_auto_deploy_status['log'].append(f"[7/7] ğŸ‰ Hadoopé›†ç¾¤éƒ¨ç½²å®Œæˆï¼å…± {len(servers)} ä¸ªèŠ‚ç‚¹")
    semi_auto_deploy_status['log'].append(f"[7/7]  å·²å®‰è£…çš„ç»„ä»¶: {', '.join(installItems)}å‡ä½äº/optç›®å½•ä¸‹")
    semi_auto_deploy_status['log'].append(f"[7/7]  è¯·åœ¨ä¸»èŠ‚ç‚¹{servers[0]['hostname']}ä¸Šæ‰§è¡Œsource ~/.hadoop_envå‘½ä»¤ï¼Œä½¿ç¯å¢ƒå˜é‡ç”Ÿæ•ˆ")
    semi_auto_deploy_status['progress'] = 100
    semi_auto_deploy_status['status'] = 'done'
    semi_auto_deploy_status['steps'][6]['status'] = 'done'
    
    # ç”Ÿæˆé›†ç¾¤è®¿é—®é“¾æ¥
    nn_url = f'http://{servers[0]["hostname"]}:9870'
    rm_url = f'http://{servers[0]["hostname"]}:8088'
    semi_auto_deploy_status['cluster_links'] = {
        'NameNode': nn_url,
        'ResourceManager': rm_url
    }
    semi_auto_deploy_status['log'].append(f"[6/6] ğŸŒ é›†ç¾¤Web UIå·²å°±ç»ªï¼Œä¸»èŠ‚ç‚¹: {servers[0]['hostname']}")
        
@app.route('/api/log', methods=['GET', 'POST'])
def get_log():
    logs = auto_deploy_status['log'] if isinstance(auto_deploy_status, dict) and 'log' in auto_deploy_status else []
    log_content = '\n'.join(logs)
    return Response(log_content, mimetype='text/plain')

@app.route('/api/deploy/auto/status', methods=['GET', 'POST'])
def api_deploy_auto_status():
    return jsonify(auto_deploy_status)

@app.route('/api/deploy/semi-auto/status')
def api_deploy_semi_auto_status():
    return jsonify(semi_auto_deploy_status)

@app.route('/api/deploy/auto/clear-logs', methods=['POST'])
def api_deploy_auto_clear_logs():
    """æ¸…ç©ºéƒ¨ç½²æ—¥å¿—"""
    try:
        auto_deploy_status['log'] = []
        return jsonify({'success': True, 'message': 'æ—¥å¿—å·²æ¸…ç©º'})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

def get_webhdfs_url(path, namenode_host=None):
    host = namenode_host or 'localhost'
    if host.startswith('http://'):
        host = host[len('http://'):]
    elif host.startswith('https://'):
        host = host[len('https://'):]
    
    # ç¡®ä¿è·¯å¾„æ ¼å¼æ­£ç¡®ï¼Œé¿å…åŒæ–œæ 
    if path.startswith('/'):
        path = path[1:]  # ç§»é™¤å¼€å¤´çš„æ–œæ 
    return f'http://{host}/webhdfs/v1/{path}'

@app.route('/api/hdfs/list')
def hdfs_list():
    path = request.args.get('path', '/')
    namenode_host = request.args.get('namenodeHost')
    url = get_webhdfs_url(path, namenode_host) + f'?op=LISTSTATUS&user.name=hadoop'
    try:
        resp = requests.get(url, timeout=5)
        resp.raise_for_status()
        data = resp.json()
        return jsonify({'success': True, 'files': data['FileStatuses']['FileStatus']})
    except Exception as e:
        return jsonify({'success': False, 'msg': str(e)})

def replace_master_with_ip(url, ip):
    # æ›¿æ¢URLä¸­çš„ä¸»æœºåä¸ºIP
    if url.find('master') != -1:
        return re.sub(r'//master(:\d+)?', f'//{ip}\\1', url)
    else:
        return re.sub(r'//slave(:\d+)?', f'//{ip}\\1', url)

@app.route('/api/hdfs/upload', methods=['POST'])
def hdfs_upload():
    file = request.files.get('file')
    hdfs_path = request.form.get('path', '/')
    namenode_host = request.form.get('namenodeHost')
    if not file:
        return jsonify({'success': False, 'msg': 'è¯·é€‰æ‹©æ–‡ä»¶'})
    # æ­£ç¡®å¤„ç†è·¯å¾„æ‹¼æ¥ï¼Œé¿å…åŒæ–œæ 
    if hdfs_path == '/':
        full_path = f'/{file.filename}'
    else:
        full_path = f'{hdfs_path}/{file.filename}'
    url = get_webhdfs_url(full_path, namenode_host) + f'?op=CREATE&overwrite=true&user.name=hadoop'
    if namenode_host:
        url = replace_master_with_ip(url, namenode_host.split(':')[0])
    try:
        resp = requests.put(url, allow_redirects=False)
        if resp.status_code in (307, 201):
            upload_url = resp.headers['Location'] if 'Location' in resp.headers else url
            # å¯¹è·³è½¬åœ°å€ä¹Ÿåšä¸»æœºåæ›¿æ¢
            if namenode_host:
                upload_url = replace_master_with_ip(upload_url, namenode_host.split(':')[0])
            resp2 = requests.put(upload_url, data=file.stream, headers={'Content-Type': 'application/octet-stream'})
            resp2.raise_for_status()
            return jsonify({'success': True})
        else:
            return jsonify({'success': False, 'msg': resp.text})
    except Exception as e:
        return jsonify({'success': False, 'msg': str(e)})

@app.route('/api/hdfs/download')
def hdfs_download():
    hdfs_path = request.args.get('path', '/')
    namenode_host = request.args.get('namenodeHost')
    url = get_webhdfs_url(hdfs_path, namenode_host) + f'?op=OPEN&user.name=hadoop'
    if namenode_host:
        url = replace_master_with_ip(url, namenode_host.split(':')[0])
    try:
        resp = requests.get(url, allow_redirects=False, stream=True)
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è·³è½¬
        if resp.status_code == 307 and 'Location' in resp.headers:
            download_url = resp.headers['Location']
            if namenode_host:
                download_url = replace_master_with_ip(download_url, namenode_host.split(':')[0])
            resp2 = requests.get(download_url, stream=True)
            resp2.raise_for_status()
            file_data = BytesIO(resp2.content)
        else:
            resp.raise_for_status()
            file_data = BytesIO(resp.content)
        return send_file(file_data, download_name=hdfs_path.split('/')[-1], as_attachment=True)
    except Exception as e:
        return f'ä¸‹è½½å¤±è´¥: {e}', 500

@app.route('/api/hdfs/delete', methods=['POST'])
def hdfs_delete():
    data = request.get_json()
    path = data.get('path')
    recursive = data.get('recursive', False)
    namenode_host = data.get('namenodeHost')
    if not path:
        return jsonify({'success': False, 'msg': 'å‚æ•°é”™è¯¯'})
    url = get_webhdfs_url(path, namenode_host) + f'?op=DELETE&recursive={'true' if recursive else 'false'}&user.name=hadoop'
    try:
        resp = requests.delete(url)
        resp.raise_for_status()
        result = resp.json()
        if result.get('boolean'):
            return jsonify({'success': True})
        else:
            return jsonify({'success': False, 'msg': 'åˆ é™¤å¤±è´¥'})
    except Exception as e:
        return jsonify({'success': False, 'msg': str(e)})

@app.route('/api/hdfs/create', methods=['POST'])
def hdfs_create():
    data = request.get_json()
    path = data.get('path')
    namenode_host = data.get('namenodeHost')
    type_ = data.get('type', 'file')
    if not path or not namenode_host:
        return jsonify({'success': False, 'msg': 'å‚æ•°é”™è¯¯'})
    if type_ == 'directory':
        url = get_webhdfs_url(path, namenode_host) + f'?op=MKDIRS&user.name=hadoop'
        try:
            resp = requests.put(url)
            resp.raise_for_status()
            return jsonify({'success': True})
        except Exception as e:
            return jsonify({'success': False, 'msg': str(e)})
    else:  # æ–°å»ºç©ºæ–‡ä»¶
        url = get_webhdfs_url(path, namenode_host) + f'?op=CREATE&overwrite=false&user.name=hadoop'
        if namenode_host:
            url = replace_master_with_ip(url, namenode_host.split(':')[0])
        try:
            resp = requests.put(url, allow_redirects=False)
            if resp.status_code in (307, 201):
                upload_url = resp.headers['Location'] if 'Location' in resp.headers else url
                # å¯¹è·³è½¬åœ°å€ä¹Ÿåšä¸»æœºåæ›¿æ¢
                if namenode_host:
                    upload_url = replace_master_with_ip(upload_url, namenode_host.split(':')[0])
                resp2 = requests.put(upload_url, data=b'', headers={'Content-Type': 'application/octet-stream'})
                resp2.raise_for_status()
                return jsonify({'success': True})
            else:
                return jsonify({'success': False, 'msg': resp.text})
        except Exception as e:
            return jsonify({'success': False, 'msg': str(e)})

if __name__ == '__main__':
    app.run(debug=True,host='0.0.0.0',port=5000)
